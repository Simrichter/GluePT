This Project was created as part of my bachelor thesis at TU Dortmund.
The code resembles a complete pipeline for pretraining GPT models on subsets of the OpenWebText dataset and finetuning them to the nine tasks of the GLUE benchmark.
Evaluation, plotting and creating the zip files necessary for submissions to the benchmark is also handled.

For an in-depth explanation of Generative Pretrained Transformers regarding their architecture, use-cases and implementation, as well as the results my models achieved, have a look inside my thesis.
